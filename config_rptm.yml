# NOTE n.1 : If sampler_type "random" or "balanced" is selected, batch_size must be divisible by num_instances
# NOTE n.2 : If sampler_type "random" or "balanced" is selected, loss type must be TripletLoss
# NOTE n.3 : If use_rptm is True, loss must be TripletLossRPTM

reid:
  # === MISC CONFIGURATION ===
  misc:
    seed: 2047315 # Seed for reproducibility
    output_dir: "results"
    use_amp: False
  # === DATASET CONFIGURATION ===
  dataset:
    data_path: "data"
    dataset_name: "veri_776" # veri_776 / veri_wild / vehicle_id
    dataset_size: "small"
    sampler_type: "" # random / balanced / "" (which is: None)
    num_instances: 6 # Only to be adjusted if sampler_type is either "random" or "balanced"

  # === MODEL CONFIGURATION ===
  model:
    model: "resnet50_ibn_a" # "resnet50" / "resnet101" / "resnet50_ibn_a" / "resnet101_ibn_a"
    pretrained: True
    device: "cuda:0"
    use_gem: False
    use_stride: True
    use_bottleneck: True

  # === AUGMENTATION CONFIGURATION ===
  augmentation:
    height: 320
    width: 320
    random_crop: [320, 320]
    random_horizontal_flip_prob: 0.5
    jitter_brightness: 0.2
    jitter_contrast: 0.15
    jitter_saturation: 0.0
    jitter_hue: 0.0
    color_augmentation: True
    padding: 0
    normalize_mean: null # [0.485, 0.456, 0.406]
    normalize_std: null # [0.229, 0.224, 0.225]
    random_erasing_prob: 0.5
    
  # === LOSS CONFIGURATION ===
  loss:
    type: "TripletLossRPTM" # TripletLoss / TripletLossRPTM / SupConLoss / SupConLoss_Pytorch / TripletMarginLoss_Pytorch
    # ===> RPTM configuration
    use_rptm: [True, "mean"]
    # ===> TripletLoss configuration
    margin: 1.00
    # ===> Label Smoothing configuration
    label_smoothing: 0.00
    # ===> MALW configuration
    apply_MALW: False
    alpha: 0.9
    k: 2000

  # === TRAINING CONFIGURATION ===
  training:
    epochs: 160
    batch_size: 24
    num_workers: 0
    output_dir: "results"
    use_amp: False
    # ===> Optimizer configuration
    optimizer: "sgd" # adam / sgd
    learning_rate: 1.0e-3
    bias_lr_factor: 2
    weight_decay: 0.0005
    weight_decay_bias: 0.0001
    # ===> Scheduler configuration (WarmupDecayLR or MultiStepLR)
    use_warmup: False
    steps: [20, 40, 60, 100, 120]
    gamma: 0.25
    warmup_epochs: 10
    decay_method: "linear" # "linear" / "smooth" / "cosine"
    cosine_power: 0.45 # Only to be adjusted if decay_method is "cosine"
    min_lr: 1.0e-6
    # ===> Logging configuration
    log_interval: 100
    # ===> Loading checkpoint
    load_checkpoint: False # Could be also a path to a checkpoint

  # === VALIDATION CONFIGURATION ===
  validation:
    batch_size: 100
    val_interval: 1
    re_ranking: True
    visualize_ranks: False

  # === TEST CONFIGURATION ===
  test:
    testing: False # Always set to False (except in config_test.yml)
    normalize_embeddings: False
    run_reid_metrics: True
    model_val_path: ""